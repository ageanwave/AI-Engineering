{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbuylkrFVhDa"
      },
      "source": [
        "<a href=\"http://cocl.us/pytorch_link_top\">\n",
        "    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/Pytochtop.png\" width=\"750\" alt=\"IBM Product \">\n",
        "</a> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp2Jt4htVhDd"
      },
      "source": [
        "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"200\" alt=\"cognitiveclass.ai logo\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQtzjBzDVhDe"
      },
      "source": [
        "<h1><h1>Pre-trained-Models with PyTorch </h1>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtrzUbOFVhDf"
      },
      "source": [
        "In this lab, you will use pre-trained models to classify between the negative and positive samples; you will be provided with the dataset object. The particular pre-trained model will be resnet18; you will have three questions: \n",
        "<ul>\n",
        "<li>change the output layer</li>\n",
        "<li> train the model</li> \n",
        "<li>  identify  several  misclassified samples</li> \n",
        " </ul>\n",
        "You will take several screenshots of your work and share your notebook. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79enJ3BuVhDg"
      },
      "source": [
        "<h2>Table of Contents</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7DZtQS1VhDg"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
        "\n",
        "\n",
        "<ul>\n",
        "    <li><a href=\"#download_data\"> Download Data</a></li>\n",
        "    <li><a href=\"#auxiliary\"> Imports and Auxiliary Functions </a></li>\n",
        "    <li><a href=\"#data_class\"> Dataset Class</a></li>\n",
        "    <li><a href=\"#Question_1\">Question 1</a></li>\n",
        "    <li><a href=\"#Question_2\">Question 2</a></li>\n",
        "    <li><a href=\"#Question_3\">Question 3</a></li>\n",
        "</ul>\n",
        "<p>Estimated Time Needed: <strong>120 min</strong></p>\n",
        " </div>\n",
        "<hr>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gw6SI2pVhDh"
      },
      "source": [
        "<h2 id=\"download_data\">Download Data</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1zpidKlVhDh"
      },
      "source": [
        "Download the dataset and unzip the files in your data directory, unlike the other labs, all the data will be deleted after you close  the lab, this may take some time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vonUD40yVhDh",
        "outputId": "ed95d65b-fb94-48b5-e4d4-98658d181858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-21 10:55:20--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2598656062 (2.4G) [application/zip]\n",
            "Saving to: ‘Positive_tensors.zip’\n",
            "\n",
            "Positive_tensors.zi 100%[===================>]   2.42G  28.8MB/s    in 81s     \n",
            "\n",
            "2023-02-21 10:56:42 (30.5 MB/s) - ‘Positive_tensors.zip’ saved [2598656062/2598656062]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r1jh3msVhDi"
      },
      "outputs": [],
      "source": [
        "!unzip -q Positive_tensors.zip "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mK_IyCOTVhDj",
        "outputId": "b41753aa-2333-468f-be86-a4d1a68c10c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-21 10:58:42--  https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
            "Resolving s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)... 67.228.254.196\n",
            "Connecting to s3-api.us-geo.objectstorage.softlayer.net (s3-api.us-geo.objectstorage.softlayer.net)|67.228.254.196|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2111408108 (2.0G) [application/zip]\n",
            "Saving to: ‘Negative_tensors.zip’\n",
            "\n",
            "Negative_tensors.zi 100%[===================>]   1.97G  28.6MB/s    in 71s     \n",
            "\n",
            "2023-02-21 10:59:53 (28.6 MB/s) - ‘Negative_tensors.zip’ saved [2111408108/2111408108]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/images/Negative_tensors.zip\n",
        "!unzip -q Negative_tensors.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PUet9BAVhDj"
      },
      "source": [
        "We will install torchvision:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Eplux1UVhDj",
        "outputId": "9a8b4947-1ece-4cca-b1ed-62ef3713f694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.1+cu116)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchvision) (4.5.0)\n",
            "Requirement already satisfied: torch==1.13.1 in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.13.1+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79f5vjDRVhDk"
      },
      "source": [
        "<h2 id=\"auxiliary\">Imports and Auxiliary Functions</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-B9FiaVhDk"
      },
      "source": [
        "The following are the libraries we are going to use for this lab. The <code>torch.manual_seed()</code> is for forcing the random function to give the same number every time we try to recompile it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNYonWzXVhDk",
        "outputId": "190c8d3b-5571-4386-ba90-ba9f5c67bb0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc27c6b9b70>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# These are the libraries will be used for this lab.\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "import pandas\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch \n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import os\n",
        "import glob\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ9kLWyEVhDk"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pylab as plt\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-K4UUe-VhDl"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNDHGAfDVhDl"
      },
      "source": [
        "<h2 id=\"data_class\">Dataset Class</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eab2vFxFVhDl"
      },
      "source": [
        " This dataset class is essentially the same dataset you build in the previous section, but to speed things up, we are going to use tensors instead of jpeg images. Therefor for each iteration, you will skip the reshape step, conversion step to tensors and normalization step.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDboNSczVhDl",
        "outputId": "1800a4ae-c025-4570-e451-577c11a11035"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "# Create your own dataset object\n",
        "\n",
        "class Dataset(Dataset):\n",
        "\n",
        "    # Constructor\n",
        "    def __init__(self,transform=None,train=True):\n",
        "        directory=\"/content/\"\n",
        "        positive=\"Positive_tensors\"\n",
        "        negative='Negative_tensors'\n",
        "\n",
        "        positive_file_path=os.path.join(directory,positive)\n",
        "        negative_file_path=os.path.join(directory,negative)\n",
        "        positive_files=[os.path.join(positive_file_path,file) for file in os.listdir(positive_file_path) if file.endswith(\".pt\")]\n",
        "        negative_files=[os.path.join(negative_file_path,file) for file in os.listdir(negative_file_path) if file.endswith(\".pt\")]\n",
        "        number_of_samples=len(positive_files)+len(negative_files)\n",
        "        self.all_files=[None]*number_of_samples\n",
        "        self.all_files[::2]=positive_files\n",
        "        self.all_files[1::2]=negative_files \n",
        "        # The transform is goint to be used on image\n",
        "        self.transform = transform\n",
        "        #torch.LongTensor\n",
        "        self.Y=torch.zeros([number_of_samples]).type(torch.LongTensor)\n",
        "        self.Y[::2]=1\n",
        "        self.Y[1::2]=0\n",
        "        \n",
        "        if train:\n",
        "            self.all_files=self.all_files[0:30000]\n",
        "            self.Y=self.Y[0:30000]\n",
        "            self.len=len(self.all_files)\n",
        "        else:\n",
        "            self.all_files=self.all_files[30000:]\n",
        "            self.Y=self.Y[30000:]\n",
        "            self.len=len(self.all_files)     \n",
        "       \n",
        "    # Get the length\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    # Getter\n",
        "    def __getitem__(self, idx):\n",
        "               \n",
        "        image=torch.load(self.all_files[idx])\n",
        "        y=self.Y[idx]\n",
        "                  \n",
        "        # If there is any transform method, apply it onto the image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, y\n",
        "    \n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EOWEw-qVhDm"
      },
      "source": [
        "We create two dataset objects, one for the training data and one for the validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MLHCk8yVhDm",
        "outputId": "bf0a4b2e-619d-4f0e-f4ff-65ae72298498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "train_dataset = Dataset(train=True)\n",
        "validation_dataset = Dataset(train=False)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilask2d5VhDn"
      },
      "source": [
        "<h2 id=\"Question_1\">Question 1</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq1v2TVeVhDn"
      },
      "source": [
        "<b>Prepare a pre-trained resnet18 model :</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwzXjOkPVhDo"
      },
      "source": [
        "<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "b04765baf0164162ac25fe8a2519fa2e",
            "fd6fc1534fbc4edcb66268d0576fc95d",
            "a69d1979c6c746a6a9f1302247a5e4e2",
            "a482094655a54f9195abf7eeaccbfb1b",
            "3e717c8f7512414cae3697b6b2154d12",
            "28886ea8173c4029b3dd59ba423f2bb0",
            "effdbd50c5c54ef0b8ccf19dab20af19",
            "be90994fbc684401bf6f085f0a654e7c",
            "60ec8815b3eb43a8b9043d7df2ccb63e",
            "0bc8319b625344bc8b5126bc9ca5cb3c",
            "28ac2cc208af4cd8994a5602d201b5aa"
          ]
        },
        "id": "D5d2HymiVhDo",
        "outputId": "bd9c7bd4-5710-4dea-c538-1b39fb73e00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b04765baf0164162ac25fe8a2519fa2e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Step 1: Load the pre-trained model resnet18\n",
        "model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAm14xbqVhDp"
      },
      "source": [
        "<b>Step 2</b>: Set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoP9rKjAVhDp"
      },
      "outputs": [],
      "source": [
        "# Step 2: Set the parameter cannot be trained for the pre-trained model\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cg2iAyESVhDq"
      },
      "source": [
        "<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSlNIcR3VhDq"
      },
      "source": [
        "<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 2 different classes. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzVOglTsVhDr"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BvJdSViVhDr"
      },
      "source": [
        "Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18AcDdB0VhDr",
        "outputId": "85a325fa-bc3f-4f0f-d225-365e23e5def1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH4vB7tXVhDr"
      },
      "source": [
        "<h2 id=\"Question_2\">Question 2: Train the Model</h2>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HyrK3cpVhDs"
      },
      "source": [
        "In this question you will train your, model:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_evYkPAVhDs"
      },
      "source": [
        "<b>Step 1</b>: Create a cross entropy criterion function \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0do6dS9VhDs"
      },
      "outputs": [],
      "source": [
        "# Step 1: Create the loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiWnu70RVhDs"
      },
      "source": [
        "<b>Step 2</b>: Create a training loader and validation loader object, the batch size should have 100 samples each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfHfH6g6VhDt"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset = train_dataset, batch_size = 100)\n",
        "validation_loader = DataLoader(dataset = train_dataset, batch_size = 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBC8GckWVhDt"
      },
      "source": [
        "<b>Step 3</b>: Use the following optimizer to minimize the loss \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSEiA0DoVhDt"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam([parameters for parameters in model.parameters() if parameters.requires_grad],lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evKQCRBWVhDt"
      },
      "source": [
        "<!--Empty Space for separating topics-->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWaNVEsAVhDu"
      },
      "source": [
        "**Complete the following code to calculate  the accuracy on the validation data for one epoch; this should take about 45 minutes. Make sure you calculate the accuracy on the validation data.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BV9XGWiDVhDu"
      },
      "outputs": [],
      "source": [
        "n_epochs=1\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "correct=0\n",
        "N_test=len(validation_dataset)\n",
        "N_train=len(train_dataset)\n",
        "start_time = time.time()\n",
        "#n_epochs\n",
        "\n",
        "Loss=0\n",
        "start_time = time.time()\n",
        "for epoch in range(n_epochs):\n",
        "    loss_sublist = []\n",
        "    for x, y in train_loader:\n",
        "\n",
        "        model.train() \n",
        "        #clear gradient \n",
        "        optimizer.zero_grad()\n",
        "        #make a prediction \n",
        "        z = model(x)\n",
        "        # calculate loss \n",
        "        loss = criterion(z,y)\n",
        "        loss_sublist.append(loss.data.item())\n",
        "        # calculate gradients of parameters \n",
        "        loss.backward()\n",
        "        # update parameters \n",
        "        optimizer.step()\n",
        "        loss_list.append(loss.data)\n",
        "    \n",
        "    correct=0\n",
        "    for x_test, y_test in validation_loader:\n",
        "        # set model to eval \n",
        "        model.eval()\n",
        "        #make a prediction \n",
        "        z = model(x_test)\n",
        "        #find max \n",
        "        _, yhat = torch.max(z.data, 1)\n",
        "       \n",
        "        #Calculate misclassified  samples in mini-batch \n",
        "        #hint +=(yhat==y_test).sum().item()\n",
        "        correct+=(yhat==y_test).sum().item()\n",
        "   \n",
        "    accuracy = correct/N_test\n",
        "    accuracy_list.append(accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kLW9AsDVhDu"
      },
      "source": [
        "<b>Print out the Accuracy and plot the loss stored in the list <code>loss_list</code> for every iteration and take a screen shot.</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3KXDr-gVhDv",
        "outputId": "4c556f08-434d-4281-e789-71ae6f3c7bae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9944"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "NsktTJv2VhDv",
        "outputId": "1ea56b35-2876-4c8b-ee9f-cb8f02a0b8e5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuZM4XvrVhDv"
      },
      "source": [
        "<h2 id=\"Question_3\">Question 3:Find the misclassified samples</h2> \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qniYMCgVhDv"
      },
      "source": [
        "<b>Identify the first four misclassified samples using the validation data:</b>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1dISxW4VhDv"
      },
      "outputs": [],
      "source": [
        "misclassified = []\n",
        "with torch.no_grad():\n",
        "    for x_test, y_test in validation_loader:\n",
        "        outputs = model(x_test)\n",
        "        _, yhat = torch.max(outputs, 1)\n",
        "        for i in range(x_test.size(0)):\n",
        "            if yhat[i] != y_test[i]:\n",
        "                misclassified.append((x_test[i], yhat[i], y_test[i]))\n",
        "                if len(misclassified) == 4:\n",
        "                    break\n",
        "        if len(misclassified) == 4:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxVpFeKLVhDw",
        "outputId": "9b385225-2073-4757-ce19-758bbe497fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misclassified sample 1: predicted 0, actual 1\n",
            "Misclassified sample 2: predicted 0, actual 1\n",
            "Misclassified sample 3: predicted 0, actual 1\n",
            "Misclassified sample 4: predicted 0, actual 1\n"
          ]
        }
      ],
      "source": [
        "for i, (x_test, yhat, y_test) in enumerate(misclassified):\n",
        "    print(f\"Misclassified sample {i+1}: predicted {yhat}, actual {y_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew_srC-xVhDw"
      },
      "source": [
        "<a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/share-notebooks.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\"> CLICK HERE </a> Click here to see how to share your notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTju9OSUVhDw"
      },
      "source": [
        "<h2>About the Authors:</h2> \n",
        "\n",
        "<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS1Ty-kVVhDw"
      },
      "source": [
        "\n",
        "## Change Log\n",
        "\n",
        "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
        "|---|---|---|---|\n",
        "| 2020-09-21  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
        "\n",
        "\n",
        "\n",
        "<hr>\n",
        "\n",
        "## <h3 align=\"center\"> © IBM Corporation 2020. All rights reserved. <h3/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yktxi9EoVhDx"
      },
      "source": [
        "Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">MIT License</a>.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b04765baf0164162ac25fe8a2519fa2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd6fc1534fbc4edcb66268d0576fc95d",
              "IPY_MODEL_a69d1979c6c746a6a9f1302247a5e4e2",
              "IPY_MODEL_a482094655a54f9195abf7eeaccbfb1b"
            ],
            "layout": "IPY_MODEL_3e717c8f7512414cae3697b6b2154d12"
          }
        },
        "fd6fc1534fbc4edcb66268d0576fc95d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28886ea8173c4029b3dd59ba423f2bb0",
            "placeholder": "​",
            "style": "IPY_MODEL_effdbd50c5c54ef0b8ccf19dab20af19",
            "value": "100%"
          }
        },
        "a69d1979c6c746a6a9f1302247a5e4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be90994fbc684401bf6f085f0a654e7c",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60ec8815b3eb43a8b9043d7df2ccb63e",
            "value": 46830571
          }
        },
        "a482094655a54f9195abf7eeaccbfb1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc8319b625344bc8b5126bc9ca5cb3c",
            "placeholder": "​",
            "style": "IPY_MODEL_28ac2cc208af4cd8994a5602d201b5aa",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 72.7MB/s]"
          }
        },
        "3e717c8f7512414cae3697b6b2154d12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28886ea8173c4029b3dd59ba423f2bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "effdbd50c5c54ef0b8ccf19dab20af19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be90994fbc684401bf6f085f0a654e7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60ec8815b3eb43a8b9043d7df2ccb63e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0bc8319b625344bc8b5126bc9ca5cb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28ac2cc208af4cd8994a5602d201b5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}